{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "MODEL = \"meta-llama/Llama-3.2-1B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_best_device\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.float16)\n",
    "device = get_best_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,     16,     10,     17,     28]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "current = tokenizer(\"1+2=\", return_tensors=\"pt\").to(device)\n",
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_probs: tensor([[0.3066, 0.3652, 0.4668, 0.2539, 0.4766]], device='mps:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<MaxBackward0>)\n",
      "torch.Size([1, 5, 128256])\n",
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0938,  9.0625, 13.3750,  ..., -3.7656, -3.7656, -3.7656],\n",
      "         [ 8.7500, 11.0625,  9.6875,  ..., -2.4219, -2.4219, -2.4219],\n",
      "         [10.3750,  7.0000,  8.7500,  ..., -1.2578, -1.2578, -1.2656],\n",
      "         [12.8750, 12.3125, 11.8750,  ..., -0.6797, -0.6797, -0.6797],\n",
      "         [10.9375,  7.8750, 10.3750,  ..., -0.5742, -0.5742, -0.5742]]],\n",
      "       device='mps:0', dtype=torch.bfloat16, grad_fn=<LinearBackward0>), past_key_values=DynamicCache(), hidden_states=None, attentions=None)\n",
      "tensor([[18]], device='mps:0')\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "past_key_values = DynamicCache()\n",
    "cache_position = torch.arange(current.input_ids.shape[1], dtype=torch.int64, device=model.device)\n",
    "outputs = model(\n",
    "            input_ids=current.input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            cache_position=cache_position\n",
    "        )\n",
    "probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "top_probs, _ = torch.max(probs, dim=-1)\n",
    "print(\"top_probs:\", top_probs,)\n",
    "\n",
    "print(outputs.logits.shape)\n",
    "next_token_logits = outputs.logits[:, -1, :]\n",
    "print(outputs)\n",
    "next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "print(next_token_id)\n",
    "pred = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "print(pred)\n",
    "past_key_values = outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[942]], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     16,     10,     17,     28,    942]], device='mps:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = tokenizer.encode(\"son\", return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "print(next_token)\n",
    "# append the new token to the input\n",
    "current = torch.cat([current, next_token], dim=-1)\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Hello, what's your name. \n",
      "<|assistant|>\n",
      "MynameisSarah.\n",
      "<|"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "past_key_values = DynamicCache()\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n",
    "\n",
    "generated_ids = inputs.input_ids\n",
    "cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=model.device)\n",
    "max_new_tokens = 10\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True), flush=True, end=\"\")\n",
    "\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n",
    "    # Greedily sample one next token\n",
    "    next_token_ids = outputs.logits[:, -1:].argmax(-1)\n",
    "    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n",
    "\n",
    "    # Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token\n",
    "    # and expanding attn mask for the new token, as explained above\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "    inputs = {\"input_ids\": next_token_ids, \"attention_mask\": attention_mask}\n",
    "    cache_position = cache_position[-1:] + 1 # add one more position for the next token\n",
    "    print(tokenizer.decode(next_token_ids[0], skip_special_tokens=True), flush=True, end=\"\")\n",
    "\n",
    "# print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Hi and welcome \n",
      "<|assistant|>\n",
      "waiting \n",
      "<|user|>\n",
      " to tech support. \n",
      "<|assistant|>\n",
      "waiting \n",
      "<|user|>\n",
      " For sales, \n",
      "<|assistant|>\n",
      "waiting \n",
      "<|user|>\n",
      " please press 1. \n",
      "<|assistant|>\n",
      "Hello world \n",
      "<|user|>\n",
      " Press 2 for \n",
      "<|assistant|>\n",
      "Hello world \n",
      "<|user|>\n",
      " for support. \n",
      "<|assistant|>\n",
      "waiting \n",
      "<|user|>\n",
      " If you require \n",
      "<|assistant|>\n",
      "waiting \n",
      "<|user|>\n",
      " support with your billing. \n",
      "<|assistant|>\n",
      "waiting \n",
      "<|user|>\n",
      " Please press 3. \n",
      "<|assistant|>\n",
      "Hello world \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi and welcome\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"waiting\"},\n",
    "    {\"role\": \"user\", \"content\": \" to tech support.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"waiting\"},\n",
    "    {\"role\": \"user\", \"content\": \" For sales,\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"waiting\"},\n",
    "    {\"role\": \"user\", \"content\": \" please press 1.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello world\"},\n",
    "    {\"role\": \"user\", \"content\": \" Press 2 for\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello world\"},\n",
    "    {\"role\": \"user\", \"content\": \" for support.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"waiting\"},\n",
    "    {\"role\": \"user\", \"content\": \" If you require\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"waiting\"},\n",
    "    {\"role\": \"user\", \"content\": \" support with your billing.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"waiting\"},\n",
    "    {\"role\": \"user\", \"content\": \" Please press 3.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello world\"},\n",
    "]\n",
    "formatted_chat = tokenizer.apply_chat_template(EXAMPLE, tokenize=True, return_dict=True, add_generation_prompt=False)\n",
    "chat_text = tokenizer.decode(formatted_chat.input_ids, skip_special_tokens=True)\n",
    "print(chat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-titans-Xm6IIKMy-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
